Aqui est√° o **Documento 2: Arquitetura T√©cnica e Stack Tecnol√≥gica**.

Este documento foi desenhado para ser entregue ao seu L√≠der T√©cnico e equipe de engenharia. Ele detalha n√£o apenas as tecnologias escolhidas, mas a estrat√©gia de integra√ß√£o entre o legado (Go/Next.js) e a nova camada de IA (Python).

---

# üìÑ Documento 2: Arquitetura T√©cnica & Stack Tecnol√≥gica
## Projeto: Snack Prompt 2.0 ‚Äì AI Engine & Agent Runtime

### 1. Vis√£o Geral da Arquitetura (O Modelo H√≠brido)

Para entregar funcionalidades de IA de ponta sem reescrever o backend est√°vel atual, adotaremos o padr√£o **Sidecar Microservice**.

*   **Core Backend (Legado - Go):** Continua sendo a "Fonte da Verdade". Gerencia Usu√°rios, Autentica√ß√£o, Pagamentos (Cr√©ditos), CRUD de Tabelas/Prompts e o banco Postgres.
*   **AI Engine (Novo - Python):** Um servi√ßo dedicado focado exclusivamente em processamento vetorial, orquestra√ß√£o de LLMs e ingest√£o de dados complexos.

#### Diagrama de Fluxo de Dados
```mermaid
[Frontend Next.js]
       |
       | (1. Auth & Business Logic)
       v
[Core Backend (Go) + Postgres]
       |
       | (2. Async Events: "Table Updated")
       | (3. Credit Check & Token Signing)
       v
[AI Engine (Python FastAPI)] <---> [Vector DB (Qdrant)]
       |
       +---> [LLMs Providers (OpenAI, Anthropic)]
       +---> [Embedding Provider (Jina AI)]
```

---

### 2. A Stack Tecnol√≥gica (O "O Que" e o "Porqu√™")

Abaixo, a justificativa t√©cnica para cada decis√£o, focada em **Performance**, **Escalabilidade** e **Time-to-Market**.

#### 2.1. Vector Database: **Qdrant**
*   **Por que n√£o PGVector?** Embora o PGVector seja conveniente, o **Qdrant** √© nativo para vetores (escrito em Rust).
    *   **Filtragem de Metadados:** Precisamos filtrar buscas por `user_id`, `table_id` e `agent_id` com milissegundos de lat√™ncia. O Qdrant faz isso melhor que o Postgres em escala.
    *   **Busca H√≠brida (Hybrid Search):** O Qdrant suporta nativamente a busca por **Vetores Densos** (sem√¢ntica) + **Vetores Esparsos** (palavras-chave exatas). Isso √© vital para KB corporativa (ex: buscar um CNPJ espec√≠fico ou c√≥digo de produto).
    *   **Quantiza√ß√£o:** Permite compress√£o de vetores para economizar at√© 4x de RAM/Disco.

#### 2.2. Embedding Model: **Jina Embeddings v3**
*   **Por que Jina v3?** √â o estado da arte (SOTA) em modelos open-source multil√≠ngues.
*   **Matryoshka Representation:** Permite "cortar" o vetor (ex: usar 512 dimens√µes em vez de 1024) mantendo 95%+ da precis√£o. Isso reduz drasticamente o custo de armazenamento no Qdrant.
*   **Task Adapters:** Podemos instruir o modelo se estamos indexando uma "Fato Jur√≠dico" ou uma "Descri√ß√£o de Produto", melhorando a qualidade do RAG.

#### 2.3. Data Pipeline (ETL): **LlamaIndex + Docling**
*   **LlamaIndex:** Ser√° o nosso "Bibliotec√°rio". Ele possui parsers hier√°rquicos (`HierarchicalNodeParser`) que s√£o perfeitos para transformar a estrutura JSON da Snack Prompt (`Tabela > Coluna > C√©lula`) em vetores sem perder o contexto do "pai".
*   **Docling (IBM):** Para quando o usu√°rio fizer upload de arquivos (PDF, DOCX). O Docling √© superior em extrair tabelas e layouts complexos de documentos, convertendo-os para Markdown estruturado antes do embedding.

#### 2.4. Agent Runtime: **LangChain (LangGraph)**
*   **LangGraph:** Ser√° o nosso "C√©rebro". Diferente de cadeias lineares, o LangGraph opera como uma M√°quina de Estados (State Machine).
*   **Por que √© essencial?** Para criar Agentes que podem ter loops (ex: "Pesquise na base -> N√£o achou? -> Reformule a pergunta -> Pesquise de novo"). Al√©m disso, ele gerencia a **Mem√≥ria (Checkpointers)** para chats longos.

---

### 3. Detalhamento dos M√≥dulos do AI Engine (Python)

O servi√ßo Python ter√° dois grandes m√≥dulos: **Ingest√£o** e **Infer√™ncia**.

#### 3.1. M√≥dulo de Ingest√£o (Writer)
Respons√°vel por manter o Qdrant sincronizado com o Postgres/JSON.

*   **Estrat√©gia de Sincronia:** `Delete-Insert` (Substitui√ß√£o At√¥mica).
*   **O Gatilho:** Quando o usu√°rio salva uma tabela no Frontend, o Go envia o payload JSON para o Python.
*   **Fluxo L√≥gico:**
    1.  Recebe JSON da Tabela.
    2.  Identifica o ID do Item (C√©lula) que mudou.
    3.  **LlamaIndex:** Deleta vetores antigos no Qdrant com `filter: { snack_item_id: "xyz" }`.
    4.  **Processamento:** Converte o conte√∫do da c√©lula em texto rico (adicionando o nome da coluna e da tabela no texto para contexto).
    5.  **Jina AI:** Gera embeddings.
    6.  **Qdrant Upsert:** Grava os novos vetores com o Payload rico.

**Estrutura do Payload no Qdrant:**
```json
{
  "payload": {
    "snack_table_id": "Table_123",
    "snack_column_id": "Col_Age",
    "snack_item_id": "Cell_ABC", // Chave para updates
    "text_content": "Contexto: Idade do Cliente. Valor: 35 anos.",
    "source_url": "https://snackprompt...",
    "access_level": "private" // Para controle de permiss√£o
  }
}
```

#### 3.2. M√≥dulo de Infer√™ncia (Reader / Chat)
Respons√°vel por executar o Agente e gerar respostas.

*   **Feature: RAG H√≠brido (Multi-Tenant)**
    Quando um usu√°rio usa um "Agente Advogado" (que tem uma KB secreta) para ler o "Contrato do Usu√°rio" (KB do usu√°rio), o Python configura o Retriever do Qdrant assim:
    
    ```python
    # Pseudoc√≥digo do Filtro Qdrant
    filter = Filter(
        should=[
            FieldCondition(key="table_id", match=MatchValue(value="KB_DO_USUARIO_ID")),
            FieldCondition(key="table_id", match=MatchValue(value="KB_SECRETA_AGENTE_ID"))
        ]
    )
    ```
    Isso permite que a IA "enxergue" ambas as bases simultaneamente.

*   **Feature: Cita√ß√µes (Citations)**
    O LangChain ser√° configurado para retornar n√£o apenas a string de resposta, mas os objetos `source_documents`. O Backend Python processar√° esses objetos para retornar ao Frontend um array de cita√ß√µes linkadas aos `snack_item_id`.

---

### 4. Arquitetura de Seguran√ßa e Prote√ß√£o de IP ("Caixa Preta")

Como garantir que o usu√°rio use a API para falar com um Agente pago sem roubar o prompt ou ver os dados secretos?

**O fluxo "Proxy Blindado":**

1.  **Request:** O cliente (ou n8n) chama `POST api.snackprompt.com/v1/chat` (Go).
2.  **Valida√ß√£o (Go):**
    *   Verifica se o usu√°rio comprou o Agente.
    *   Verifica se o usu√°rio tem **Cr√©ditos** suficientes.
    *   Gera um `execution_token` tempor√°rio contendo os IDs permitidos.
3.  **Execu√ß√£o (Python):**
    *   O Go repassa a request para o Python.
    *   O Python recupera o **System Prompt** (descriptografado em mem√≥ria).
    *   O Python usa a **API Key da Snack Prompt** (n√£o a do usu√°rio) para chamar a OpenAI.
    *   **Output:** O Python retorna *apenas* a resposta textual. Os logs da OpenAI ficam na conta da Snack Prompt, inacess√≠veis ao usu√°rio.
4.  **Billing (Go):**
    *   Ap√≥s a resposta, o Python informa ao Go: "Gastei 500 tokens".
    *   O Go debita os cr√©ditos da carteira do usu√°rio.

---

### 5. Integra√ß√£o com Modelos (Model Factory)

No servi√ßo Python, teremos uma *Factory* para instanciar o LLM correto baseado no plano:

```python
def get_llm(user_context, agent_config):
    # Cen√°rio 1: Agente "Caixa Preta" (Usa infra Snack Prompt + Cr√©ditos)
    if agent_config.is_blackbox:
        return ChatOpenAI(api_key=PLATFORM_KEY, model="gpt-4o")
    
    # Cen√°rio 2: BYOK (Usu√°rio traz a chave)
    if user_context.has_custom_key:
        return ChatOpenAI(api_key=user_context.decrypted_key, model="gpt-4o")
        
    # Cen√°rio 3: Plano Free
    return ChatOpenAI(api_key=PLATFORM_KEY, model="gpt-4o-mini")
```

---

### 6. Considera√ß√µes de Frontend (Next.js)

Para suportar essa nova arquitetura, o Frontend precisar√° de:

1.  **Componente de Chat com Streaming:** Usar a biblioteca `ai` da Vercel ou `langchain-adapter` para suportar *Server-Sent Events (SSE)* e dar a sensa√ß√£o de digita√ß√£o em tempo real.
2.  **Renderizador de Markdown + Cita√ß√µes:** Um parser que detecta a tag `[Ref: 1]` e a transforma em um Tooltip ou Link clic√°vel que abre a gaveta lateral com a c√©lula da tabela.
3.  **Agent Builder UI:** Uma interface "Drag-and-Drop" ou formul√°rio passo-a-passo para o usu√°rio selecionar Prompts existentes e Tabelas para compor um Agente.

### Resumo do Plano de Execu√ß√£o T√©cnica

1.  **Setup Infra:** Subir container Qdrant e criar o repo do FastAPI.
2.  **Ingest√£o v1:** Criar webhook no Go -> Python para indexar tabelas simples.
3.  **Retrieval v1:** Endpoint b√°sico de chat (Python) conversando com OpenAI.
4.  **Agentes:** Implementar a l√≥gica de LangGraph e o filtro de RAG H√≠brido.
5.  **Billing/API:** Implementar a l√≥gica de cr√©ditos e o gateway para uso externo.

Esta arquitetura √© **Cloud-Native**, escala horizontalmente (o Python pode ter N r√©plicas pois √© stateless, o estado fica no Postgres/Qdrant) e utiliza o que h√° de mais moderno em IA (Vetores + Grafos).